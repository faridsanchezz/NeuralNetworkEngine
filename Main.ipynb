{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NeuronalNetwork import NeuralLayer, NeuralNetwork\n",
    "from PerdFunc import binary_crossentropy_derivative, binary_crossentropy\n",
    "from ActivFunc import relu, relu_derivative, softmax, softmax_derivative\n",
    "from Optimizer import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neural_network(X_train, y_train):\n",
    "    # Arquitectura de la red neuronal\n",
    "    input_size = X_train.shape[1]\n",
    "    output_size = y_train.shape[1]\n",
    "    hidden_size1 = 256\n",
    "    hidden_size2 = 128\n",
    "\n",
    "    # Creación de capas\n",
    "    input_layer = NeuralLayer(input_size, hidden_size1, relu, relu_derivative)\n",
    "    hidden_layer1 = NeuralLayer(hidden_size1, hidden_size2, relu, relu_derivative)\n",
    "    hidden_layer2 = NeuralLayer(hidden_size2, output_size, softmax, softmax_derivative, use_bias=False)\n",
    "\n",
    "    # Creación de la red neuronal\n",
    "    neural_network = NeuralNetwork([input_layer, hidden_layer1, hidden_layer2])\n",
    "    return neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(neural_network, X_train, y_train, X_test, y_test, optimizer, epochs=20, batch_size=64, regularization_strength=0.001, momentum=0.9):\n",
    "    # Entrenamiento\n",
    "    for epoch in range(epochs):\n",
    "        indices = np.arange(X_train.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for i in range(0, X_train.shape[0], batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            X_batch = X_train[batch_indices]\n",
    "            y_batch = y_train[batch_indices]\n",
    "\n",
    "            # Forward pass\n",
    "            predicted_output = neural_network.forward(X_batch)\n",
    "\n",
    "            # Cálculo de la pérdida\n",
    "            loss = np.mean(binary_crossentropy(y_batch, predicted_output))\n",
    "\n",
    "            # Backpropagation\n",
    "            error = binary_crossentropy_derivative(y_batch, predicted_output)\n",
    "            neural_network.backward(error, optimizer, regularization_strength, momentum)\n",
    "\n",
    "        # Evaluación en el conjunto de prueba\n",
    "        test_output = neural_network.forward(X_test)\n",
    "        test_loss = np.mean(binary_crossentropy(y_test, test_output))\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}, Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_final_accuracy(neural_network, X_test, y_test):\n",
    "    # Evaluación final en el conjunto de prueba\n",
    "    final_output = neural_network.forward(X_test)\n",
    "    final_accuracy = np.mean(np.argmax(final_output, axis=1) == np.argmax(y_test, axis=1))\n",
    "    print(f\"Final Accuracy on Test Set: {final_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos nuestra red para el conjunto MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.datasets import mnist\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "#\n",
    "# def load_and_preprocess_data_mnist():\n",
    "#     # Carga y preprocesamiento de datos (MNIST)\n",
    "#     (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#     X_train = X_train.reshape((X_train.shape[0], -1)) / 255.0\n",
    "#     X_test = X_test.reshape((X_test.shape[0], -1)) / 255.0\n",
    "#     y_train = to_categorical(y_train)\n",
    "#     y_test = to_categorical(y_test)\n",
    "#     return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos para el conjunto de datos cifar-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.datasets import cifar10\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "#\n",
    "# def load_and_preprocess_data_cifar_10():\n",
    "#     # Carga y preprocesamiento de datos (cifar-100)\n",
    "#     (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "#     X_train = X_train.reshape((X_train.shape[0], -1)) / 255.0\n",
    "#     X_test = X_test.reshape((X_test.shape[0], -1)) / 255.0\n",
    "#     y_train = to_categorical(y_train)\n",
    "#     y_test = to_categorical(y_test)\n",
    "#     return X_train, y_train, X_test, y_test\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Carga y preprocesamiento de datos\n",
    "    X_train, y_train, X_test, y_test = load_and_preprocess_data_mnist()\n",
    "\n",
    "    # Construcción de la red neuronal\n",
    "    neural_network = build_neural_network(X_train, y_train)\n",
    "\n",
    "    # Entrenamiento de la red neuronal\n",
    "    optimizer = SGD(learning_rate=0.01)\n",
    "    train_neural_network(neural_network, X_train, y_train, X_test, y_test, optimizer, epochs=20, batch_size=64, regularization_strength=0.001, momentum=0.9)\n",
    "\n",
    "    # Evaluación final en el conjunto de prueba\n",
    "    evaluate_final_accuracy(neural_network, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 34s 0us/step\n",
      "Epoch 1/20, Loss: 0.2342, Test Loss: 0.2575\n",
      "Epoch 2/20, Loss: 0.2170, Test Loss: 0.2445\n",
      "Epoch 3/20, Loss: 0.2158, Test Loss: 0.2394\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
